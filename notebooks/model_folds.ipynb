{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1f3d5a-11b0-4037-9d8b-ec5fde219d57",
   "metadata": {},
   "source": [
    "# Articles classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c41a24-4d27-41bf-804a-0ad77c00c502",
   "metadata": {},
   "source": [
    "## Libraries and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45669f8b-ccf0-4850-a5be-b3a9b6b7eeb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b49de91-9aa6-45d7-a775-1eca2923c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModel, \n",
    "    BertTokenizer, \n",
    "    BertForSequenceClassification,\n",
    "    AdamW\n",
    ")\n",
    "from sklearn.metrics import  (\n",
    "    f1_score, \n",
    "    accuracy_score, \n",
    "    multilabel_confusion_matrix, \n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bafc133-e6be-4cae-b530-856e10d998ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "VER = 'v0'\n",
    "DATA_PATH = './data'\n",
    "MDLS_PATH = f'/home/jovyan/__LABELING/models_{VER}'\n",
    "CONFIG = {\n",
    "    'data_file': 'project-6-at-2024-03-13-08-22-16c5be41.json',\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'bbone': 'DeepPavlov/rubert-base-cased-sentence',\n",
    "    'targets_type': 'explicit', # targets can be 'all', explicit', 'general' \n",
    "    'targets': [\n",
    "        'target_0_explicit', \n",
    "        'target_3_explicit', \n",
    "        'target_3_general', \n",
    "        'target_4_explicit',\n",
    "        'target_4_general',\n",
    "        'target_7_explicit',\n",
    "        'target_7_general', \n",
    "        'target_11_explicit', \n",
    "        'target_11_general',\n",
    "        'target_12_explicit', \n",
    "        'target_12_general'\n",
    "    ], \n",
    "    'targets_description': {\n",
    "        'target_0': 'ЦУР отсутствуют', \n",
    "        'target_3': 'ЦУР 3 - Хорошее здоровье и благополучие',\n",
    "        'target_4': 'ЦУР 4 - Качественное образование',\n",
    "        'target_7': 'ЦУР 7 - Недорогостоящая и чистая энергия', \n",
    "        'target_11': 'ЦУР 11 - Устойчивые города',\n",
    "        'target_12': 'ЦУР 12 - Ответственное потребление и производство', \n",
    "    },\n",
    "    'folds': 5,\n",
    "    'max_seq_len': 512,\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 4,\n",
    "    'acc': False,\n",
    "    'epochs': 50,\n",
    "    'lr': 2e-5, # default `2e-5`\n",
    "    'patience': 5,\n",
    "    'seed': 23\n",
    "}\n",
    "if not os.path.exists(MDLS_PATH):\n",
    "    os.mkdir(MDLS_PATH)\n",
    "with open(f'{MDLS_PATH}/config.json', 'w') as file:\n",
    "    json.dump(CONFIG, file)\n",
    "\n",
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "\n",
    "seed_all(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef169c2-be28-4302-9d8b-c9855cd049cd",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0dce51-cc92-4799-b81a-07f6b4a70539",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DATA_PATH}/{CONFIG[\"data_file\"]}') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print('total records:', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f6eb7-380f-4bb1-84d8-40428db063c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def last_ann(anns):\n",
    "    return sorted(anns, key=lambda d: d['updated_at'])[-1]\n",
    "\n",
    "ds = []\n",
    "for d in data:\n",
    "    dd = {}\n",
    "    dd['idart'] = d['id']\n",
    "    dd['anno'] = d['data']['anno']\n",
    "    dd.update(\n",
    "        {\n",
    "            x['from_name'] + '_' + x['value']['choices'][0] : 1\n",
    "            for x in last_ann(d['annotations'])['result']\n",
    "        }\n",
    "    )\n",
    "    ds.append(dd)\n",
    "df = pd.DataFrame(ds)\n",
    "df.fillna(0, inplace=True)\n",
    "seq_len = [len(str(i).split()) for i in df['anno']]\n",
    "print('max sequence lenght:', max(seq_len))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c392e96-8c65-4c06-acc1-573f14fbc155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chk_balance(df, target_cols):\n",
    "    print('total:', len(df))\n",
    "    for col in target_cols:\n",
    "        if 'target' in col: print(\n",
    "            col, '\\t',\n",
    "            df[col].sum(), '\\t',\n",
    "            '{:.1%}'.format(df[col].sum() / len(df))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbef4b88-ce99-4b75-b579-9553ee209ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = sorted(list(set([\n",
    "    c.replace('_explicit', '').replace('_general', '') \n",
    "    for c in CONFIG['targets']\n",
    "])))\n",
    "if CONFIG['targets_type'] == 'explicit':\n",
    "    for col in target_cols:\n",
    "        df[col] = df[col + '_explicit']\n",
    "elif CONFIG['targets_type'] == 'general':\n",
    "    for col in target_cols:\n",
    "        name1 = col + '_explicit'\n",
    "        name2 = col + '_general'\n",
    "        if col == 'target_0':\n",
    "            df[col] = df[name1]\n",
    "        else:\n",
    "            df[col] = df.apply(lambda x: max(x[name1], x[name2]), axis=1)\n",
    "elif CONFIG['targets_type'] == 'all':\n",
    "    target_cols = [col for col in df.columns if 'target' in col]\n",
    "else:\n",
    "    ValueError('`targets_type` parameter error')\n",
    "if CONFIG['targets_type'] != 'all':\n",
    "    df.loc[\n",
    "        df[[c for c in target_cols if 'target_0' not in c]].sum(axis=1) == 0, \n",
    "        'target_0'\n",
    "    ] = 1\n",
    "chk_balance(df, target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2a06c6-dcd0-4939-a120-dda7eabd6421",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG['target_cols'] = target_cols\n",
    "with open(f'{MDLS_PATH}/config.json', 'w') as file:\n",
    "    json.dump(CONFIG, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d8bd8-c590-4ebf-a711-f1643d52af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(CONFIG['folds'], shuffle=True, random_state=CONFIG['seed'])\n",
    "df['fold'] = -1\n",
    "for i, (train_idxs, val_idxs) in enumerate(skf.split(df, df['target_0_explicit'])):\n",
    "    df.loc[val_idxs, 'fold'] = i\n",
    "for fold_num in range(CONFIG['folds']): \n",
    "    train_idxs = np.where((df['fold'] != fold_num))[0]\n",
    "    val_idxs = np.where((df['fold'] == fold_num))[0]\n",
    "    df_train = df.loc[train_idxs]\n",
    "    df_val = df.loc[val_idxs]\n",
    "    print('FOLD', fold_num)\n",
    "    chk_balance(df_train, target_cols)\n",
    "    chk_balance(df_val, target_cols)\n",
    "    print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f6b3d-2dc3-409b-ab11-7428f56a594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0056562-899f-439a-9b9d-8d04f9d9e155",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcfc587-1ad0-4906-a753-a6ce3079c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, model_name, target_cols):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.rubert = AutoModel.from_pretrained(model_name)\n",
    "        #self.l2 = torch.nn.Dropout(.3)\n",
    "        self.fc = torch.nn.Linear(768, len(target_cols))\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, features = self.rubert(\n",
    "            ids, \n",
    "            attention_mask=mask, \n",
    "            token_type_ids=token_type_ids, \n",
    "            return_dict=False\n",
    "        )\n",
    "        #output_2 = self.l2(output_1)\n",
    "        output = self.fc(features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06398b8-4fc9-4c21-bc21-ac7ae97a7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, target_cols):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.text = df['anno']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.targets = df[target_cols].values if target_cols else []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606fc5c2-c164-4f1c-b37b-977603314178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArticlesTrainer:\n",
    "    def __init__(self, model, device, \n",
    "                 optimizer, scheduler, \n",
    "                 criterion, acc_flag=True):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.criterion = criterion\n",
    "        self.acc_flag = acc_flag\n",
    "        if acc_flag:\n",
    "            self.best_val_acc = 0\n",
    "        else:\n",
    "            self.best_val_loss = np.inf\n",
    "        self.val_losses = []\n",
    "        self.train_losses = []\n",
    "        self.val_acc = []\n",
    "        self.lastmodel = None\n",
    "        \n",
    "    def fit(self, epochs, train_loader, val_loader, \n",
    "            max_patience, save_mode='best', save_name='model'):     \n",
    "        n_patience = 0\n",
    "        for n_epoch in range(1, epochs + 1):\n",
    "            self.info_message('EPOCH: {}', n_epoch)\n",
    "            train_loss, train_time = self.train_epoch(train_loader)\n",
    "            val_loss, val_acc, val_f1_mic, val_f1_mac, val_time = self.val_epoch(val_loader)\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_acc.append(val_acc)\n",
    "            self.info_message(\n",
    "                'epoch train: {} | loss: {:.2f} | time: {:.0f} sec',\n",
    "                n_epoch, train_loss, train_time\n",
    "            )\n",
    "            self.info_message(\n",
    "                'epoch val: {} | loss: {:.2f} | ' +\n",
    "                'acc: {:.2f} | f1 micro: {:.2f} | f1 macro: {:.2f} | ' +\n",
    "                'time: {:.0f} sec',\n",
    "                n_epoch, val_loss, val_acc, val_f1_mic, val_f1_mac, val_time\n",
    "            )\n",
    "            if self.acc_flag:\n",
    "                if self.best_val_acc < val_acc: \n",
    "                    self.save_model(n_epoch, save_mode, save_name, val_loss, val_acc)\n",
    "                    self.info_message(\n",
    "                        'val accuracy improved {:.2f} -> {:.2f} | saved model to \"{}\"', \n",
    "                        self.best_val_acc, val_acc, self.lastmodel\n",
    "                    )\n",
    "                    self.best_val_acc = val_acc\n",
    "                    n_patience = 0\n",
    "                else:\n",
    "                    n_patience += 1\n",
    "            else:\n",
    "                if self.best_val_loss > val_loss: \n",
    "                    self.save_model(n_epoch, save_mode, save_name, val_loss, val_acc)\n",
    "                    self.info_message(\n",
    "                        'val loss improved {:.2f} -> {:.2f} | saved model to \"{}\"', \n",
    "                        self.best_val_loss, val_loss, self.lastmodel\n",
    "                    )\n",
    "                    self.best_val_loss = val_loss\n",
    "                    n_patience = 0\n",
    "                else:\n",
    "                    n_patience += 1\n",
    "            if n_patience >= max_patience:\n",
    "                self.info_message(\n",
    "                    '\\nno improvement for last {} epochs', \n",
    "                    n_patience\n",
    "                )\n",
    "                break\n",
    "        history = {\n",
    "            'train losses': self.train_losses, \n",
    "            'val losses': self.val_losses, \n",
    "            'val accuracy': self.val_acc\n",
    "        }\n",
    "        return history\n",
    "            \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        t = time.time()\n",
    "        sum_loss = 0\n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            ids = batch['ids'].to(self.device, dtype=torch.long)\n",
    "            mask = batch['mask'].to(self.device, dtype=torch.long)\n",
    "            token_type_ids = batch['token_type_ids'].to(self.device, dtype=torch.long)\n",
    "            targets = batch['targets'].to(self.device, dtype=torch.float)\n",
    "            self.optimizer.zero_grad() \n",
    "            outputs = self.model(ids, mask, token_type_ids)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if self.scheduler: \n",
    "                self.scheduler.step()\n",
    "            sum_loss += loss.detach().item()   \n",
    "            self.info_message(\n",
    "                'train step {}/{} | train loss: {:.4f}           ',\n",
    "                step, len(train_loader), sum_loss / step, end='\\r'\n",
    "            )\n",
    "        return sum_loss / len(train_loader), int(time.time() - t)\n",
    "    \n",
    "    def val_epoch(self, val_loader):\n",
    "        self.model.eval()\n",
    "        t = time.time()\n",
    "        sum_loss = 0\n",
    "        y_all = []\n",
    "        outputs_all = []\n",
    "        for step, batch in enumerate(val_loader, 1):\n",
    "            with torch.no_grad():\n",
    "                ids = batch['ids'].to(self.device, dtype=torch.long)\n",
    "                mask = batch['mask'].to(self.device, dtype=torch.long)\n",
    "                token_type_ids = batch['token_type_ids'].to(self.device, dtype=torch.long)\n",
    "                targets = batch['targets'].to(self.device, dtype=torch.float)\n",
    "                outputs = self.model(ids, mask, token_type_ids)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                sum_loss += loss.detach().item()\n",
    "                y_all.extend(targets.cpu().detach().numpy().tolist())\n",
    "                outputs_all.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "            self.info_message(\n",
    "                'val step {}/{} | val loss: {:.4f}               ', \n",
    "                step, len(val_loader), sum_loss / step, end='\\r'\n",
    "            )\n",
    "        outputs_all = np.array(outputs_all) > .5\n",
    "        acc = accuracy_score(y_all, outputs_all)\n",
    "        f1_mic = f1_score(y_all, outputs_all, average='micro')\n",
    "        f1_mac = f1_score(y_all, outputs_all, average='macro')\n",
    "        return sum_loss / len(val_loader), acc, f1_mic, f1_mac, int(time.time() - t)\n",
    "    \n",
    "    def save_model(self, n_epoch, save_mode, save_name, loss, acc):\n",
    "        if save_mode == 'best':\n",
    "            self.lastmodel = f'{MDLS_PATH}/{save_name}.pth'\n",
    "        else:\n",
    "            self.lastmodel = f'{MDLS_PATH}/{save_name}-e{n_epoch}-loss{loss:.3f}-acc{acc:.3f}.pth'\n",
    "        dict_save = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'n_epoch': n_epoch,\n",
    "        }\n",
    "        if self.acc_flag:\n",
    "            dict_save['best_val_acc'] = self.best_val_acc\n",
    "        else:\n",
    "            dict_save['best_val_loss'] = self.best_val_loss\n",
    "        torch.save(dict_save, self.lastmodel)\n",
    "    \n",
    "    def display_plots(self):\n",
    "        fig, axes = plt.subplots(figsize=(16, 4), nrows=1, ncols=2)\n",
    "        axes[0].set_title(f'training and validation losses')\n",
    "        axes[0].plot(self.val_losses, label='val')\n",
    "        axes[0].plot(self.train_losses, label='train')\n",
    "        axes[0].set_xlabel('iterations')\n",
    "        axes[0].set_ylabel('loss')\n",
    "        axes[0].legend()\n",
    "        axes[1].set_title(f'validation accuracy')\n",
    "        axes[1].plot(self.val_acc, label='val')\n",
    "        axes[1].set_xlabel('iterations')\n",
    "        axes[1].set_ylabel('accuracy')\n",
    "        axes[1].legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    @staticmethod\n",
    "    def info_message(message, *args, end='\\n'):\n",
    "        print(message.format(*args), end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da02211a-f5ec-4bf3-8c98-e711f3b75f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_art_model(df_train, df_val, target_cols,\n",
    "                    device, model_name, max_seq_len,\n",
    "                    epochs, save_name, patience, \n",
    "                    batch_size, num_workers):\n",
    "    print('=' * 20, f'MODEL TRAIN - {save_name}', '=' * 20)\n",
    "    print('train:', df_train.shape, '| val:', df_val.shape)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    train_dataset = ArticlesDataset(\n",
    "        df=df_train, \n",
    "        tokenizer=tokenizer, \n",
    "        max_len=max_seq_len, \n",
    "        target_cols=target_cols\n",
    "    )\n",
    "    val_dataset = ArticlesDataset(\n",
    "        df=df_val, \n",
    "        tokenizer=tokenizer, \n",
    "        max_len=max_seq_len, \n",
    "        target_cols=target_cols\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers, \n",
    "        shuffle=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=num_workers, \n",
    "        shuffle=False, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    model = BERTClass(\n",
    "        model_name=model_name, \n",
    "        target_cols=target_cols\n",
    "    )\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=1e-6)\n",
    "    scheduler = None\n",
    "    criterion = nn.functional.binary_cross_entropy_with_logits\n",
    "    trainer = ArticlesTrainer(\n",
    "        model, \n",
    "        device, \n",
    "        optimizer, \n",
    "        scheduler,\n",
    "        criterion,\n",
    "        acc_flag=CONFIG['acc']\n",
    "    )\n",
    "    history = trainer.fit(\n",
    "        epochs, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        save_mode='best', \n",
    "        save_name=save_name,\n",
    "        max_patience=patience\n",
    "    )\n",
    "    trainer.display_plots()\n",
    "    with open(f'{MDLS_PATH}/history_{save_name}.json', 'w') as file:\n",
    "        json.dump(history, file)\n",
    "    return trainer.lastmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2704c-8d6b-47e4-95db-316ac6f91997",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = []\n",
    "for fold_num in range(CONFIG['folds']): \n",
    "    train_idxs = np.where((df['fold'] != fold_num))[0]\n",
    "    val_idxs = np.where((df['fold'] == fold_num))[0]\n",
    "    df_train = df.loc[train_idxs]\n",
    "    df_val = df.loc[val_idxs]\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "    df_val.reset_index(drop=True, inplace=True)\n",
    "    model_files.append(train_art_model(\n",
    "        df_train, \n",
    "        df_val, \n",
    "        target_cols,\n",
    "        device=CONFIG['device'], \n",
    "        model_name=CONFIG['bbone'],\n",
    "        max_seq_len=CONFIG['max_seq_len'],\n",
    "        epochs=CONFIG['epochs'], \n",
    "        save_name=f'model_{fold_num}',\n",
    "        patience=CONFIG['patience'], \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        num_workers=CONFIG['num_workers']\n",
    "    ))\n",
    "print(model_files)\n",
    "with open(f'{MDLS_PATH}/model_files.json', 'w') as file:\n",
    "    json.dump(model_files, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f67ab-002f-42ee-a86e-e0c735384705",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a61862-37d4-42b1-b54f-e1b1fae1e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{MDLS_PATH}/model_files.json', 'r') as file:\n",
    "    model_files = json.load(file)\n",
    "print(\"models files loaded:\", model_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd869b8-e1ba-486e-b9f0-89c7f86bb54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model_file, df, target_cols, \n",
    "          model_name, max_seq_len,\n",
    "          device, batch_size, num_workers):\n",
    "    print('PREDICT:', model_file, df.shape)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    pred_dataset = ArticlesDataset(\n",
    "        df=df, \n",
    "        tokenizer=tokenizer, \n",
    "        max_len=max_seq_len, \n",
    "        target_cols=target_cols\n",
    "    )\n",
    "    pred_loader = torch.utils.data.DataLoader(\n",
    "        pred_dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=num_workers, \n",
    "        shuffle=False, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    model = BERTClass(\n",
    "        model_name=model_name, \n",
    "        target_cols=target_cols\n",
    "    )\n",
    "    model.to(device)\n",
    "    checkpoint = torch.load(model_file)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    for i, batch in enumerate(pred_loader, start=1):\n",
    "        print(f'prediction step {i}/{len(pred_loader)}   ', end='\\r')\n",
    "        with torch.no_grad():\n",
    "            ids = batch['ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
    "            tmp_pred = model(ids, mask, token_type_ids)\n",
    "            y_pred.extend(torch.sigmoid(tmp_pred).cpu().detach().numpy().tolist())\n",
    "    df_pred = pd.DataFrame(y_pred) \n",
    "    df_pred.columns = [c + '_pred' for c in target_cols]\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40dc7e-3f6c-4509-8afd-d97d7b0c4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame()\n",
    "for model_file in tqdm(model_files):\n",
    "    df_pred_tmp = infer(\n",
    "        model_file=model_file, \n",
    "        df=df_val, \n",
    "        target_cols=target_cols,\n",
    "        model_name=CONFIG['bbone'],\n",
    "        max_seq_len=CONFIG['max_seq_len'],\n",
    "        device=CONFIG['device'], \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        num_workers=CONFIG['num_workers']\n",
    "    )\n",
    "    if len(df_pred):\n",
    "        df_pred += df_pred_tmp #.add(df_pred_tmp, fill_value=0)\n",
    "    else:\n",
    "        df_pred = df_pred_tmp\n",
    "df_pred /= len(model_files)\n",
    "display(df_pred.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a60296-54f9-4e53-ae42-32db1c11419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unique = target_cols\n",
    "mcm = multilabel_confusion_matrix(\n",
    "    (df_pred > .5).values, \n",
    "    df_val[target_cols].values\n",
    ")\n",
    "for c, cm in zip(target_cols, mcm):\n",
    "    print(\n",
    "        c, '\\t',\n",
    "        df_val[c].sum(), '\\t',\n",
    "            '{:.1%}'.format(df[c].sum() / len(df)),\n",
    "    )\n",
    "    print(cm)\n",
    "    print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e8b3d-e2ed-4c89-a112-ad99a77ea52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b6ef92-9a20-4760-a129-87e2bf9fe7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Цель настоящего исследования заключается в изучении рисков от изменения климата в \n",
    "городской системе Стамбула. Стамбул является крупнейшим городом в Турции по численности\n",
    "населения и наличию экономических возможностей, в связи с чем любой связанный с климатом \n",
    "риск будет разрушительным не только для города, но и для страны в целом. Городская система \n",
    "определяется на основе городских секторов, которые являются центрами деловой активности, \n",
    "управления, экологических систем, ресурсов и имеют решающее значение для жизнеспособности \n",
    "экономики и общественного здоровья города. Это также делает их уязвимыми перед климатическими \n",
    "катастрофами. С учетом стратегий развития Стамбула в области водных ресурсов, \n",
    "здравоохранения, энергетики, сельского хозяйства, транспорта, развития и землепользования, \n",
    "общественной безопасности, инфраструктуры, биоразнообразия и экологии, культуры, \n",
    "материалов определены 11 городских секторов и 25 подсекторов. Руководство МСМЭИ (ICLEI-Local \n",
    "Governments for Sustainability - Международный совет по местным экологическим инициативам) \n",
    "редоставило инструкцию по оценке рисков для этих территорий и секторов «Подготовка \n",
    "к изменению климата: руководство для местных, региональных и национальных правительств». \n",
    "Данные получены в ходе глубинных интервью с городскими стейкхолдерами. Секторы ранжированы \n",
    "с учетом факторов риска для каждого из них. Результаты исследования позволили выявить \n",
    "городские секторы, подверженные большему и меньшему риску из-за последствий изменения климата. \n",
    "Определение рисков от изменения климата в жизненно важных секторах Стамбула является \n",
    "необходимым при принятии решений для разработки дальнейших стратегий по смягчению возможных \n",
    "последствий и адаптации к новым условиям.\"\"\"\n",
    "text = df_val.loc[1, \"anno\"]\n",
    "print(text[:50], len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb724b-5eb9-4f5a-9329-9fa478dbdefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'anno': [text]}\n",
    "d.update(dict(zip(target_cols, [0] * len(target_cols))))\n",
    "df_txt = pd.DataFrame(d)\n",
    "df_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d49622-d912-4542-806d-b468c033208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_pred_tmp = infer(\n",
    "    model_file=model_files[0], \n",
    "    df=df_txt, \n",
    "    target_cols=target_cols,\n",
    "    model_name=CONFIG['bbone'],\n",
    "    max_seq_len=CONFIG['max_seq_len'],\n",
    "    device=CONFIG['device'], \n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=CONFIG['num_workers']\n",
    ")\n",
    "df_pred_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84192303-89df-41c4-b225-577e90d7721a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
