{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d4e2f6-91c9-4130-ad5e-70c8c08ddafd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef928e-2978-4a1f-87fc-4a62fa9fddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModel, \n",
    "    BertTokenizer, \n",
    "    BertForSequenceClassification,\n",
    "    AdamW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4adda4-29cc-4b13-8272-c00e5429aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path) as file:\n",
    "        access_data = json.load(file)\n",
    "    return access_data\n",
    "\n",
    "CREDS = read_json(file_path='../configs/access_bucket.json')\n",
    "print(CREDS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38850f30-6fa2-4d21-a309-36abb325e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "s3 = session.client(\n",
    "    service_name='s3',\n",
    "    aws_access_key_id=CREDS['aws_access_key_id'],\n",
    "    aws_secret_access_key=CREDS['aws_secret_access_key'],\n",
    "    endpoint_url=CREDS['endpoint_url'] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f522b-e6bd-4577-be39-559ed91df472",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x['Key'] for x in s3.list_objects(Bucket=CREDS['name'])['Contents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6521fa9e-6f28-48cc-a19a-04cc228c5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_CONFIG = read_json(file_path='../configs/config.json')\n",
    "print('application config loaded:', APP_CONFIG)\n",
    "\n",
    "file_to_load = f'{APP_CONFIG[\"model\"]}/config.json'\n",
    "get_object_response = s3.get_object(\n",
    "    Bucket=CREDS['name'], \n",
    "    Key=file_to_load\n",
    ")\n",
    "CONFIG = json.load(get_object_response['Body'])\n",
    "print('config loaded:', CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ac369-1d00-4579-a0d5-cd90a8e223ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, target_cols):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.text = df['anno']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.targets = df[target_cols].values if target_cols else []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, model_name, target_cols):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.rubert = AutoModel.from_pretrained(model_name)\n",
    "        self.fc = torch.nn.Linear(768, len(target_cols))\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, features = self.rubert(\n",
    "            ids, \n",
    "            attention_mask=mask, \n",
    "            token_type_ids=token_type_ids, \n",
    "            return_dict=False\n",
    "        )\n",
    "        output = self.fc(features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a94be9-9aa2-43b5-b551-ac9b5c6e4445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesPredictor():\n",
    "    def __init__(self, model_name, target_cols, max_seq_len,\n",
    "                 device, batch_size, num_workers, model_files):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.target_cols = target_cols\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.device = device\n",
    "        self.models = []\n",
    "        for m_name in model_files:\n",
    "            m = BERTClass(\n",
    "                model_name=model_name, \n",
    "                target_cols=target_cols\n",
    "            )\n",
    "            m.to(device)\n",
    "            get_object_response = s3.get_object(\n",
    "                Bucket=CREDS['name'], \n",
    "                Key=m_name\n",
    "            )\n",
    "            checkpoint = torch.load(\n",
    "                io.BytesIO(get_object_response['Body'].read()),\n",
    "                map_location=torch.device('cpu')\n",
    "            )\n",
    "            m.load_state_dict(checkpoint['model_state_dict'])\n",
    "            m.eval()\n",
    "            self.models.append(m)\n",
    "            print('loaded:', m_name)\n",
    "    \n",
    "    def infer(self, df):\n",
    "        print('PREDICT:', df.shape)\n",
    "        pred_dataset = ArticlesDataset(\n",
    "            df=df, \n",
    "            tokenizer=self.tokenizer, \n",
    "            max_len=self.max_seq_len, \n",
    "            target_cols=self.target_cols\n",
    "        )\n",
    "        pred_loader = torch.utils.data.DataLoader(\n",
    "            pred_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers, \n",
    "            shuffle=False, \n",
    "            pin_memory=True\n",
    "        )\n",
    "        y_pred = []\n",
    "        for j, model in enumerate(self.models):\n",
    "            y_pred_tmp = []\n",
    "            for i, batch in enumerate(pred_loader, start=1):\n",
    "                print(f'model {j}, prediction step {i}/{len(pred_loader)}   ', end='\\r')\n",
    "                with torch.no_grad():\n",
    "                    ids = batch['ids'].to(self.device, dtype=torch.long)\n",
    "                    mask = batch['mask'].to(self.device, dtype=torch.long)\n",
    "                    token_type_ids = batch['token_type_ids'].to(self.device, dtype=torch.long)\n",
    "                    tmp_pred = model(ids, mask, token_type_ids)\n",
    "                    y_pred_tmp.extend(torch.sigmoid(tmp_pred).cpu().detach().numpy().tolist())\n",
    "            y_pred.append(y_pred_tmp)\n",
    "        y_pred = np.mean(y_pred, axis=0)\n",
    "        df_pred = pd.DataFrame(y_pred) \n",
    "        df_pred.columns = [c + '_pred' for c in self.target_cols]\n",
    "        return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecee5af-c739-4ff1-a723-827512b34a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG['target_cols']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b956f99-21ad-488f-a654-7e8473554400",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_load = f'{APP_CONFIG[\"model\"]}/model_files.json'\n",
    "get_object_response = s3.get_object(\n",
    "    Bucket=CREDS['name'], \n",
    "    Key=file_to_load\n",
    ")\n",
    "MODEL_FILES = json.load(get_object_response['Body'])\n",
    "MODEL_FILES = [\n",
    "    f'{APP_CONFIG[\"model\"]}/{x.split(\"/\")[-1]}'\n",
    "    for x in MODEL_FILES\n",
    "]\n",
    "print('model files:', MODEL_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e0a790-67af-46d0-a39f-fa70ebbac122",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor = ArticlesPredictor(\n",
    "    model_name=CONFIG['bbone'], \n",
    "    target_cols=CONFIG['target_cols'], \n",
    "    max_seq_len=CONFIG['max_seq_len'],\n",
    "    device='cpu', \n",
    "    batch_size=1, \n",
    "    num_workers=1, \n",
    "    model_files=MODEL_FILES #io.BytesIO(get_object_response['Body'].read())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30266259-28fa-418a-9dd4-2b7835a45301",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Результаты исследования позволили выявить \n",
    "городские секторы, подверженные большему и меньшему риску из-за последствий изменения климата. \n",
    "Определение рисков от изменения климата в жизненно важных секторах Стамбула является \n",
    "необходимым при принятии решений для разработки дальнейших стратегий по смягчению возможных \n",
    "последствий и адаптации к новым условиям.\"\"\"\n",
    "d = {'anno': [text]}\n",
    "d.update(dict(zip(CONFIG['target_cols'], [0] * len(CONFIG['target_cols']))))\n",
    "df_txt = pd.DataFrame(d)\n",
    "df_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c72637-be10-4060-8f29-24ecc8170a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_pred = predictor.infer(df_txt)\n",
    "display(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231ad5b-a8ff-4bc6-b308-609c8088cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8640e9-8286-42e5-975d-7e17f6c4b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {}\n",
    "for k, v in df_pred.to_dict().items():\n",
    "    lbl = k.replace('_pred', '')\n",
    "    preds[lbl] = v[0]\n",
    "data = {}\n",
    "data['legend'] = CONFIG['targets_description']\n",
    "data['predictions'] = preds\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f2f7a-9994-4862-9805-c8bc2b2104b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28544b96-0b12-4e9e-b7c5-c48d69cda276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6732e4cc-e0d5-4400-9879-f2372a0b9e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6abd0-df8b-480a-b8dc-4cf833f9c71c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27174089-b1fa-4375-a40f-3a158f6e11fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
